---
title: "Lab 1: NYT API"
author: "Your Name"
date: "2023-04-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite) #convert results from API queries into R-friendly formats 
library(tidyverse)
library(tidytext) #text data management and analysis
library(ggplot2) #plot word frequencies and publication dates
library(stringr)


#assign API key.  When you create a NYT Dev account, you will be given a key
API_KEY <- "17pVCXaWVAhXng5kApAN17ErwmC8Hcrb"
```

Today we will be grabbing some data from the New York Times database via their API, then running some basic string manipulations, trying out the tidytext format, and creating some basic plots.

<https://developer.nytimes.com/>

### Connect to the New York Times API and send a query

We have to decide which New York Times articles we are interested in examining. For this exercise, I chose articles about Deb Haaland, the current US Secretary of the Interior. As a member of the Laguna Pueblo Tribe, Haaland is the first Native American to serve as Cabinet secretary. Very cool!

We'll send a query to the NY Times API using a URL that contains information about the articles we'd like to access.

fromJSON() is a wrapper function that handles our request and the API response. We'll use it to create an object,t, with the results of our query. The flatten = T argument converts from the nested JSON format to an R-friendlier form.

```{r api, eval = FALSE}

t <- fromJSON("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=haaland&api-key=",API_KEY,flatten = TRUE) 

#create the query url
url <- paste("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=haaland&api-key=",API_KEY, sep ="")

#send the request, receive the response, and flatten
t <- fromJSON(url, flatten = T)
```

```{r api, eval = FALSE}
#what type of object is t?
class(t) 

t <- data.frame(t)

# how big is it?
dim(t)

# what variables are we working with?
names(t)
#t <- readRDS("nytDat.rds") #in case of API emergency :)

```

The name format, response.xxx.xxx..., is a legacy of the JSON nested hierarchy.

Let's look at a piece of text. Our data object has a variable called "response.docs.snippet" that contains a short excerpt, or "snippet" from the article. Let's grab a snippet and try out some basic 'stringr' functions.

```{r basic_stringr, eval=FALSE}
t$response.docs.snippet[9]

#assign a snippet to x to use as fodder for stringr functions.  You can follow along using the sentence on the next line.

x <- "Her nomination as secretary of the interior is historic, but as the first Native cabinet member, she would have to strike a delicate balance." 

tolower(x) #lowercase 

str_split(x, ','); str_split(x, 't'). #split string wherever there is a comma. then split wherever there is a t. 

str_replace(x, 'historic', 'without precedent') #replace historic with without precedent 

str_replace(x, ' ', '_') #first one

#how do we replace all of them?
str_replace_all(x,' ', '_') #replace all spaces with underscores

str_detect(x, 't'); str_detect(x, 'tive') ### is pattern in the string? T/F

str_locate(x, 't'); str_locate_all(x, 'as') #find the character location of a specific value
```

### OK, it's working but we want more data. Let's set some parameters for a bigger query.

```{r}
term1 <- "Haaland" # Need to use $ to string  together separate terms
begin_date <- "20210120"
end_date <- "20220401"

#construct the query url using API operators
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term1,term2,"&begin_date=",begin_date,"&end_date=",end_date,"&facet_filter=true&api-key=","API KEY HERE", sep="")

#examine our query url
baseurl
```

The initial query returned one page of ten articles, but also gave us count of total number of hits on our query. We can use that to size a for() loop to automate requests.

```{r, eval=FALSE}
#dig into the JSON object to find total hits
initialQuery <- fromJSON(baseurl)

maxPages <- 10

#initiate a lost to hold resilts of our for loop
pages <- list()


#loop
for(i in 0:maxPages){
  nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame() 
  message("Retrieving page ", i)
  pages[[i+1]] <- nytSearch 
  Sys.sleep(12) 
}


nyt_df <- do.call(pages) #do call lets you access multiple items in list. Bind the pages and create a tibble
```

We converted each returned JSON object into a data frame

```{r, bind_rows}

saveRDS(nyt_df, "nyt_df.rds")
```

Load the preconstructed nytDat so you can follow along.

```{r article-type}
nytDat <- readRDS("nyt_df.rds")
```

```{r date-plot}
nytDat %>%
  mutate(pubDay=gsub("T.*","",response.docs.pub_date)) %>%
  group_by(pubDay) %>%
  summarise(count=n()) %>%
  filter(count >= 2) %>%
  ggplot() +
  geom_bar(aes(x=reorder(pubDay, count), y=count), stat="identity") +
  coord_flip() +
  theme_bw()
```

The New York Times doesn't make full text of the articles available through the API. But we can use the first paragraph of each article.

```{r plot_frequencies}
#columns names
names(nytDat)

paragraph <- names(nytDat)[6] #The 6th column is what we focus on here

#break data down to basic unit of analysis. tokenizing at word level here

#use tidytex::unnest token to put in tidy form 
tokenized <- nytDat %>% 
  unnest_tokens(word, paragraph) #word is the new column, paragraph is the source


tokenized[,"word"] #one token (word in this case) per row 
```


```{r word_frequencies}
tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) + 
  geom_col() +
  labs(y = NULL)
```

Uh oh, who knows what we need to do here?

We'll load a dictionary of stop words, that is words we want to exclude from our data.

```{r stop-words}
data(stop_words)

stop_words

tokenized <- tokenized %>%
  anti_join(stop_words)
```

Now we can take a look and clean up anything that seems off
```{r cleaning, eval=FALSE}
tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) + 
  geom_col() +
  labs(y = NULL)

```

```{r}
tokenized$word

clean_tokens <- str_remove_all(tokenized$word, "[:digit:]")

clean_tokens <- gsub("'s", "", clean_tokens)

tokenized$clean <- clean_tokens




tib <- subset(tokenized, clean!="") #remove mptry strings

tokenized <- tib


```




## Assignment (Due Tuesday 4/11 11:59pm)

1.  Create a free New York Times account (<https://developer.nytimes.com/get-started>)

2.  Pick an interesting environmental key word(s) and use the {jsonlite} package to query the API. Pick something high profile enough and over a large enough time frame that your query yields enough articles for an interesting examination.

3.  Recreate the publications per day and word frequency plots using the first paragraph.  This time filter on the response.docs.news_desk variable to winnow out irrelevant results.

-   Make some (at least 3) transformations to the corpus (add context-specific stopword(s), stem a key term and its variants, remove numbers)

4.  Recreate the publications per day and word frequency plots using the headlines variable (response.docs.headline.main). Compare the distributions of word frequencies between the first paragraph and headlines. Do you see any difference?

```{r api, eval = FALSE}
 
#create the query url
url <- paste("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=earthquake&api-key=",API_KEY, sep ="")

#send the request, receive the response, and flatten
t <- fromJSON(url, flatten = T)
```

```{r api, eval = FALSE}
#what type of object is t?
class(t) 
t <- data.frame(t)
# how big is it?
dim(t)
# what variables are we working with?
names(t)
```

The name format, response.xxx.xxx..., is a legacy of the JSON nested hierarchy.

### OK, it's working but we want more data. Let's set some parameters for a bigger query.

```{r}
term1 <- "earthquake" # Need to use $ to string together separate terms

begin_date <- "20230120"
end_date <- "20230401"

#construct the query url using API operators
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term1,"&begin_date=",begin_date,"&end_date=",end_date,"&facet_filter=true&api-key=","NTKBHbsb6XFEkGymGumAiba7n3uBvs8V", sep="")

#examine our query url
baseurl
```

The initial query returned one page of ten articles, but also gave us count of total number of hits on our query. We can use that to size a for() loop to automate requests.

```{r, eval=FALSE}
#dig into the JSON object to find total hits
initialQuery <- fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1) 

maxPages

#initiate a list to hold results of our for loop
pages <- list()

#loop
for(i in 0:maxPages){
  nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame() 
  message("Retrieving page ", i)
  pages[[i+1]] <- nytSearch 
  Sys.sleep(20) 
}
```

We converted each returned JSON object into a data frame

```{r, bind_rows}
class(nytSearch)
class(pages)

pages <- pages[-15] #the 15th element in the list had different columns, so I removed it to continue with the analysis



#need to bind the pages and create a tibble from nytDat
df <- do.call(what = "rbind", args = test)

saveRDS(df, "nyt_df.rds")
```

Load the preconstructed nytDat so you can follow along.

```{r article-type}
df %>% 
  group_by(response.docs.news_desk) %>%
  summarize(count=n()) %>%
  mutate(percent = (count / sum(count))*100) %>%
  ggplot() +
  geom_bar(aes(y=percent, x=response.docs.news_desk, fill=response.docs.news_desk), stat = "identity") + coord_flip() +
  theme_minimal() +
  guides(fill = FALSE) +
  labs(x = "Type of Article", y = "Percent of Articles", title = "Most articles referencing earthquakes are tagged as NYTNow and/or Foreign")
```

```{r date-plot}
df %>%
  mutate(pubDay=gsub("T.*","",response.docs.pub_date)) %>%
  group_by(pubDay) %>%
  summarise(count=n()) %>%
  filter(count >= 3) %>%
  ggplot() +
  geom_bar(aes(x=reorder(pubDay, count), y=count), stat="identity") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Date of Article", y = "Number of Articles", title = "Most articles about earthquakes took place immediately following the 7.8 magnitude Turkey-Syria earthquake.")
```

The New York Times doesn't make full text of the articles available through the API. But we can use the first paragraph of each article.

```{r plot_frequencies}
names(df)
paragraph <- names(df)[6] #The 6th column, "response.doc.lead_paragraph", is the one we want here. 

#use tidytex::unnest_tokens to put in tidy form 
tokenized <- df %>% unnest_tokens(word, paragraph) #word is the new column, paragraph is the source

tokenized[,"word"]
```


```{r stop-words}
data(stop_words)

tokenized <- tokenized %>%
  anti_join(stop_words)

tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```


```{r cleaning, eval=FALSE}
#inspect the list of tokens (words)
tokenized$word

clean_tokens <- str_remove_all(tokenized$word, "[:digit:]") #remove all numbers

clean_tokens <- gsub("’s", '', clean_tokens)

tokenized$clean <- clean_tokens

tokenized %>%
  count(clean, sort = TRUE) %>%
  mutate(clean = reorder(clean, n)) %>%
  ggplot(aes(n, clean)) +
  geom_col() +
  labs(y = NULL)
#remove the empty strings
tib <-subset(tokenized, clean!="")
#reassign
tokenized <- tib
#try again
tokenized %>%
  count(clean, sort = TRUE) %>%
  filter(n > 10) %>% 
  mutate(clean = reorder(clean, n)) %>%
  ggplot(aes(n, clean)) +
  geom_col() +
  labs(y = NULL)
```
