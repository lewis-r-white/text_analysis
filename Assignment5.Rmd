---
title: "Assignment 5"
author: "Lewis White"
date: "2023-05-12"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# LOAD PACKAGES

library(quanteda) 
library(tm)
library(topicmodels)
library(ldatuning)
library(tidyverse)
library(tidytext)
library(reshape2)

library(LexisNexisTools)
library(readr)
library(stringr)
library(here)

library(widyr)
library(irlba) 
library(broom) 
library(textdata)
```

### Assignment

####Initial set up of reading in the lexus nexus data and cleaning it for analysis
```{r, warning=FALSE, message=FALSE}
#set working directory to where the data is located 
setwd(here("data/animal_trade"))

#obtain list of files 
my_files <- list.files(pattern = ".docx", path = getwd(), full.names = TRUE, recursive = TRUE, ignore.case = TRUE)


#read in the data
dat <- lnt_read(my_files)


#get the meta data
meta_df <- dat@meta # @ referencing table in list 

#get the articles data
articles_df <- dat@articles

#get the paragraphs data
paragraphs_df <- dat@paragraphs

#create a new data frame with columns of interest
exotic_animals_df <- tibble(Date = meta_df$Date,
              Headline = meta_df$Headline,
              id = dat@articles$ID,
              text = dat@articles$Article)

#finding similar articles 
duplicates_df <- lnt_similarity(texts = dat@articles$Article,
                                dates = dat@meta$Date,
                                IDs = dat@articles$ID,
                                threshold = 0.85)

#removing similar articles 
exotic_animals_df <- anti_join(exotic_animals_df, duplicates_df, by = c("id" = "ID_duplicate"))


#manually remove a few other duplicates
exotic_animals_df <- exotic_animals_df %>%
  filter(!id %in% c(5, 6, 7, 18, 38, 40, 75))


#looking through the text column, a lot of headline/artifact content shows up as really short paragraphs, so I'm removing anything under 50 characters. 
exotic_animals_df$clean_text <- ifelse(nchar(exotic_animals_df$text) < 50, NA, exotic_animals_df$text) 

#some articles start with irrelevent text and then --, so I'm removing content before -- punctuation.
exotic_animals_df$clean_text <- gsub(".*?--", "", exotic_animals_df$text)

#a few articles came from newstex and had a description of the article and newstex before actually beginning content, so I truncated the text here as well
exotic_animals_df$clean_text <- gsub(".*Newstex", "", exotic_animals_df$clean_text)

#making sure there aren't duplicates in the clean_text column 
exotic_animals_df <- distinct(exotic_animals_df, clean_text, .keep_all = TRUE)
```


#### Train Your Own Embeddings

1.  Using the data from your Nexis Uni query from Week 2, create a set of word embeddings. To do this, you'll essentially need to recreate the steps in today's example down through the chunk named "pmi".


Starting off with unigram probabilities
```{r, warning=FALSE, message=FALSE}
unigram_probs <- exotic_animals_df %>%
  unnest_tokens(word, text) %>%
  # Remove stop words
  anti_join(stop_words, by = 'word') %>%
  # Remove numbers from the word column
  mutate(word = str_remove_all(word, "[[:digit:]]+")) %>%
  # Count frequency of each word
  count(word, sort = TRUE) %>%
  filter(word != "") %>%
  # Calculate probability of each word
  mutate(p = n/sum(n))

```

Creating an ngram with n = 5 
```{r, warning=FALSE, message=FALSE}
#skipgram creation (5 word relation)
skipgrams <- exotic_animals_df %>%
  unnest_tokens(ngram, text, token = "ngrams", n = 5) %>%
  mutate(ngramID = row_number()) %>%
  unite(skipgramsID, id, ngramID) %>% #create skipgram_ID by uniting ID and ngramID
  unnest_tokens(word, ngram) %>%
  anti_join(stop_words, by = 'word') %>%
  mutate(word = str_remove_all(word, "[[:digit:]]+")) %>%
  filter(word != "")
```

sum the total number of occurrences of pairs of words
```{r, warning=FALSE, message=FALSE}
#calculate probabilities
skipgram_probs <- skipgrams %>%
  pairwise_count(word, skipgramsID, diag = T, sort = T) %>%
  mutate(p = n/sum(n)) %>%
  filter(item1 != item2) #remove words that are matched with each other
```

normalize these probabilities
```{r, warning=FALSE, message=FALSE}
normalized_prob <- skipgram_probs %>% 
  filter(n>15) %>%
  rename(word1 = item1,
         word2 = item2) %>%
  left_join(unigram_probs %>%
              select(word1 = word,
                     p1 = p), by = "word1") %>%
  left_join(unigram_probs %>%
              select(word2 = word,
                     p2 = p), by = "word2") %>%
  mutate(p_together = p/p1/p2)
```

Calculate point-wise mutual information (PMI) measure
```{r, warning=FALSE, message=FALSE}
pmi_matrix <- normalized_prob %>%
  mutate(pmi = log10(p_together)) %>%
  cast_sparse(word1, word2, pmi)

dim(pmi_matrix)


#Limit dimensions to 100 for comuptation to run smoothly 
pmi_matrix@x[is.na(pmi_matrix@x)] <- 0

pmi_svd <- irlba(pmi_matrix, 100, maxit = 500) #1000 vectors would crash

#extract word vectors
word_vectors <- pmi_svd$u #These vectors in the "u" matrix are contain "left singular values". They are orthogonal vectors that create a 100-dimensional semantic space where we can locate each word. The distance between words in this space gives an estimate of their semantic similarity.

#rename row names to match words
rownames(word_vectors) <- rownames(pmi_matrix)
```


2.  Think of 3-5 key words in your data set. Calculate and plot the 10 most semantically similar words for each of them. Identify and interpret any interesting or surprising results.


```{r syn-function, warning=FALSE, message=FALSE}
#Here's a function written by Julia Silge for matching the most similar vectors to a given vector.
search_synonyms <- function(word_vectors, selected_vector) {
dat <- word_vectors %*% selected_vector
    
similarities <- dat %>%
        tibble(token = rownames(dat), similarity = dat[,1])
similarities %>%
       arrange(-similarity) %>%
        select(c(2,3))
}
```


```{r, warning=FALSE, message=FALSE}
library(ggplot2)

#Create function to make plots
plot_synonyms <- function(word) {
  synonyms <- search_synonyms(word_vectors, word_vectors[word, ]) %>%
    slice_max(n = 10, order_by = similarity) %>%
    mutate(token = reorder(token, similarity)) %>%
    as.data.frame()
  
  plot <- ggplot(synonyms, aes(x = similarity, y = token)) +
    geom_col(fill = "lightblue", color = "black", width = 0.6) +
    labs(x = "Similarity", y = NULL, title = paste0("Top 10 synonyms for '", word, "'")) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5),
          axis.title.y = element_text(margin = margin(r = 10)),
          axis.text.y = element_text(size = 12),
          axis.line.x = element_line(size = 0.5, color = "black"),
          panel.grid.major.y = element_line(size = 0.2, color = "gray"),
          panel.grid.minor.y = element_blank())
  
  return(plot)
}

joe_plot <- plot_synonyms("joe")

tiger_plot <- plot_synonyms("tiger")

disease_plot <- plot_synonyms("disease")

illegal_plot <- plot_synonyms("illegal")


#GRAPHING THE PLOTS TOGETHER
library(patchwork)
(joe_plot + tiger_plot) / (disease_plot + illegal_plot)

```

**Interpretation**

Given that a number of the data sets used to create the embeddings were about the Netflix show Tiger King, I was surprised that the top synonyms for Joe were not more clearly related to the show. I expected words like "Netflix", "show", "exotic", "tiger", and "king" to appear in this list. 

For the word "tiger", however, nods to the Netflix show appeared frequently. "series", "king", "netflix" and "featuring" all clearly relate to the hit show. I expected words like this to appear here, but am not sure why they are included her but not for the word "Joe".

The word "disease" had a few particularly strong synonyms ("injuries" and "respiratory"). The other synonymns (aside from the word "disease" itself and the plural form of the word) were much less similar. 

The synonyms for "illegal" seem appropriate given the context of the articles. 



3.  Assemble 3 word math equations that you think could be useful or interesting for exploring the meaning of key words or ideas in your data set.

```{r, warning=FALSE, message=FALSE}
tiger_king <- word_vectors["tiger", ] + word_vectors["king",]

search_synonyms(word_vectors, tiger_king)



exotic_disease <- word_vectors["exotic", ] + word_vectors["disease",]

search_synonyms(word_vectors, exotic_disease)



animal_not_illegal <- word_vectors["animal", ] - word_vectors["illegal",]

search_synonyms(word_vectors, animal_not_illegal)
```

**Interpretation**

When "tiger" and "king" are combined, the results are pretty strongly related to the netflix show. 

I tested "exotic" + "disease" to see whether mentions of Covid would appear, but the closest reference to Covid is the word "respiratory" (as Covid targets the respiratory system.)

The top 5 most similar words for "animal" - "illegal" make sense, as they focus on helping and protecting animals. The words "neglect", "trafficking" and "abuse" also appear in the top synonyms, which makes sense given that "animal" and "legal" would have these words appear too. 


#### Pretrained Embeddings

4.  Following the example in the SMLTR text (section 5.4), create a set of 100-dimensional GloVe word embeddings. These embeddings were trained by researchers at Stanford on 6 billion tokens from Wikipedia entries.

```{r, warning=FALSE, message=FALSE, cache=TRUE}
library(tidytext)

tns <- getNamespace("textdata")
assignInNamespace(x = "printer", value = function(...) 1, ns = tns)

#Loading in the glove embeddings (timeout 250 to ensure that they have enough time to download)
glove6b <- embedding_glove6b(dimensions = 100, options(timeout = 250))
```


5.  Test them out with the cannonical word math equation on the GloVe embeddings: "berlin" - "germany" + "france" = ?

```{r, warning=FALSE, message=FALSE}
# Convert glove6b to a matrix
embedding_matrix <- as.matrix(glove6b[, -1])

# Set row names of the matrix to be the tokens
rownames(embedding_matrix) <- glove6b$token 


#creating embedding vector for berlin - germany + france
bfg <- embedding_matrix["berlin", ] - embedding_matrix["germany",] + embedding_matrix["france",]

search_synonyms(embedding_matrix, bfg) #top result is Paris. Oui Oui it worked!
```


6.  Recreate parts 2 and 3 above using the the GloVe embeddings in place of the ones you trained. How do they compare? What are the implications for applications of these embeddings?


#### recreating part 2
```{r, warning=FALSE, message=FALSE}
#Create function to make plots
glove_plot_synonyms <- function(word) {
  synonyms <- search_synonyms(embedding_matrix, embedding_matrix[word, ]) %>%
    slice_max(n = 10, order_by = similarity) %>%
    mutate(token = reorder(token, similarity)) %>%
    as.data.frame()
  
  plot <- ggplot(synonyms, aes(x = similarity, y = token)) +
    geom_col(fill = "lightblue", color = "black", width = 0.6) +
    labs(x = "Similarity", y = NULL, title = paste0("Top 10 synonyms for '", word, "'")) +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5),
          axis.title.y = element_text(margin = margin(r = 10)),
          axis.text.y = element_text(size = 12),
          axis.line.x = element_line(size = 0.5, color = "black"),
          panel.grid.major.y = element_line(size = 0.2, color = "gray"),
          panel.grid.minor.y = element_blank())
  
  return(plot)
}

glove_joe_plot <- glove_plot_synonyms("joe")

glove_tiger_plot <- glove_plot_synonyms("tiger")

glove_disease_plot <- glove_plot_synonyms("disease")

glove_illegal_plot <- glove_plot_synonyms("illegal")


#GRAPHING THE PLOTS TOGETHER
library(patchwork)
(glove_joe_plot + glove_tiger_plot) / (glove_disease_plot + glove_illegal_plot)
```

**Interpretation**

For "joe", this time the top synonyms are other names, which makes sense given the immense corpus used here. 

For "tiger", the very top words are mostly related to tigers in general, but with "woods" and "mickelson" sneaking onto the list, it's clear that quite a few articles about golf (Tiger woods and his rival Phil Mickelson) were included. 

For "disease" synonyms for disease were included along with some common diseases. 

For "illegal", it's clear that articles related to the immigration crisis were fairly central to the corpus. 


#### Recreating part 3
```{r, warning=FALSE, message=FALSE}
glove_tiger_king <- embedding_matrix["tiger", ] + embedding_matrix["king",]

search_synonyms(embedding_matrix, glove_tiger_king)


glove_exotic_disease <- embedding_matrix["exotic", ] + embedding_matrix["disease",]

search_synonyms(embedding_matrix, glove_exotic_disease)


glove_animal_not_illegal <- embedding_matrix["animal", ] - embedding_matrix["illegal",]

search_synonyms(embedding_matrix, glove_animal_not_illegal)
```


When "tiger" and "king" are combined, the results are no longer focused on the netflix show. 

"exotic" + "disease" lead to words associated with diseases, especially illnesses related to animals. 

The top most similar words for "animal" - "illegal" are fairly scientific here. The corpus likely included a range of wikipedia articles on studying animals. 